---
layout: post
title: "Transformer表达性研究：负注意力机制"
date: 2024-12-20
categories: [transformer, Attention]
---

## 引言：传统注意力机制的局限和负向信号的缺失
注意力机制的灵感来源于人类选择性聚焦于显著细节并忽略不重要信息的能力。在数学上，注意力机制通过计算注意力权重来反映输入序列中各部分对当前任务的相对重要性，随后将这些权重应用于值向量以生成加权上下文表示。

标准的Transformer采用缩放点积注意力，其核心数学表达式为：
$$
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

其中 $Q$、$K$、$V$ 分别表示查询、键和值矩阵，$d_k$ 是键向量的维度。

$QK^T$产生一组原始的相似度分数，即logits。logits在数学上自然包含正值、零和负值。正值表示高度相似或者对齐，负值表示不相关甚至反相关。这些负值携带重要判别信息，指示哪些上下文应该被忽略或者主动排除。

softmax归一化函数$\sigma(z)_i=\frac{e^{z_i}}{\sum_j{e^{z_j}}}$对这些负信号产生了结构性的破坏，$z_i$越小，最终的权重越接近0，这种操作抹去了负值之间的相对幅度和差异性，使得模型无法区分微弱的不相关和强烈的不相关/反相关信号。

存在两个问题：
- softmax对抑制信息的结构性破坏与低效性：极度负向的logits经过softmax之后产生的极小权重可能导致来自不相关上下文的梯度信号极其微弱，使得模型难以有效学习如何主动忽略或者排斥特定的输入特征；强制权重和为1，注意力资源被强行分配给所有的上下文token，即使这些token与当前的查询任务高度不相关，本质上是一种信息守恒，这种守恒约束是一种低效的资源分配，浪费了计算资源，并可能引入噪声。打破非负约束本质上是从概率模型转变为影响因子模型，从而摆脱信息守恒的约束，提高了表示的判别能力。
- 传统注意力机制的删除操作实现不足：在标准的transformer模型中，信息删除是被动的、静态的。如果一个token $V_j$的信息需要被忽略，模型必须依赖$W_V$、$W_O$来学习一个将该token映射为零或者近似于零的静态表示。或者，模型依赖FFN在计算的更深层级来消除该信息。传统的注意力机制只能进行信息的叠加。

## 相关工作
1. Cog Attention

    使用指数函数+符号恢复，避免softmax；信息删除/抑制、表征鲁棒性

2. Repulsive Attention

    贝叶斯/粒子优化采样（SVGD/SPOS）梯度更新；避免头部坍缩、增强多头特征多样性、结构鲁棒性

## 未来计划

- 哪些模型具备有区分不相关和负相关的能力？这些模型有什么特征使得他们具备这种能力？
- DNN中有哪些工作是使用影响因子模型代替概率模型的？这么做带来了什么好处
- 哪些工作定量分析了Attention/DNN中的删除操作？

感谢你的访问，期待与你交流！

