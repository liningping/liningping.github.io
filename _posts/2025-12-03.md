---
layout: post
title: "Transformer表达性研究：负注意力机制"
date: 2025-12-03
categories: [transformer, Attention]
---

## 引言：传统注意力机制的局限和负向信号的缺失
注意力机制的灵感来源于人类选择性聚焦于显著细节并忽略不重要信息的能力。在数学上，注意力机制通过计算注意力权重来反映输入序列中各部分对当前任务的相对重要性，随后将这些权重应用于值向量以生成加权上下文表示。

标准的Transformer采用缩放点积注意力，其核心数学表达式为：
$$
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

其中 $Q$、$K$、$V$ 分别表示查询、键和值矩阵，$d_k$ 是键向量的维度。

$QK^T$产生一组原始的相似度分数，即logits。logits在数学上自然包含正值、零和负值。正值表示高度相似或者对齐，负值表示不相关甚至反相关。这些负值携带重要判别信息，指示哪些上下文应该被忽略或者主动排除。

softmax归一化函数$\sigma(z)_i=\frac{e^{z_i}}{\sum_j{e^{z_j}}}$对这些负信号产生了结构性的破坏，$z_i$越小，最终的权重越接近0，这种操作抹去了负值之间的相对幅度和差异性，使得模型无法区分微弱的不相关和强烈的不相关/反相关信号。

存在两个问题：
- softmax对抑制信息的结构性破坏与低效性：极度负向的logits经过softmax之后产生的极小权重可能导致来自不相关上下文的梯度信号极其微弱，使得模型难以有效学习如何主动忽略或者排斥特定的输入特征；强制权重和为1，注意力资源被强行分配给所有的上下文token，即使这些token与当前的查询任务高度不相关，本质上是一种信息守恒，这种守恒约束是一种低效的资源分配，浪费了计算资源，并可能引入噪声。打破非负约束本质上是从概率模型转变为影响因子模型，从而摆脱信息守恒的约束，提高了表示的判别能力。
- 传统注意力机制的删除操作实现不足：在标准的transformer模型中，信息删除是被动的、静态的。如果一个token $V_j$的信息需要被忽略，模型必须依赖$W_V$、$W_O$来学习一个将该token映射为零或者近似于零的静态表示。或者，模型依赖FFN在计算的更深层级来消除该信息。传统的注意力机制只能进行信息的叠加。

## 相关工作
1. Cog Attention

    使用指数函数+符号恢复，避免softmax；信息删除/抑制、表征鲁棒性

2. Repulsive Attention

    贝叶斯/粒子优化采样（SVGD/SPOS）梯度更新；避免头部坍缩、增强多头特征多样性、结构鲁棒性
### 研究注意力机制没有负分数带来的具体问题的定量分析
3. Confidence Regulation Neurons in Language Models
    
    探究语言模型transformer块MLP中中间隐藏层神经元中存在的两种影响最后输出logits的neuron，一类是entropy neurons，写入unembedding的有效零空间，不直接影响logits，通过影响layernormal的调整系数来调整logits的分布，达到增大不确定性的目的；一类是token frequency neurons，写入词频向量空间，词频向量就是词表元素的先验统计频率向量，调整对数词频向量的倍数来让logits靠近/远离原始词频分布，从而达到增大/降低置信度的目的。

4. Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations 
    
    潜在空间引导是作弊还是有效干预？
### 研究注意力机制没有全正分数带来的具体问题的定量分析

1. cosformer

    认为attention应该是全正的 如果有负分数会 导致产生负相关信息的聚合 这会损害表达

    希望保留：非负和重加权的特征，softmax本身是非线性化的重加权机制 本身是sharpening分布

    非负通过relu来保证；重加权是假设相邻或局部的上下文信息往往最重要，使用相对位置的余弦编码来重加权

2. GENERALIZED PROBABILISTIC ATTENTION MECHANISM IN TRANSFORMERS
    
    问题：传统注意力机制的秩坍缩和梯度消失问题，着重关注秩坍缩的问题。秩坍缩就是随着层数的增加，不同的位置的表征开始趋同，最后坍缩到一个很小的空间，通常可以用秩来衡量一个空间独立方向的数量，因此可以称为秩坍缩

    方法：softmax注意力机制是确保全部加权系数大于等于0且和为1，是一种凸组合，而凸组合会限制表达能力，最终趋于平均状态，故将原先的凸组合换成仿射组合。

    $$
    Y = P^GX_V \\
    P^G = (1+\lambda^+)P^+ + \lambda^-P^- \\
    p^- = \frac{\sigma(W_Q^-X_Q)X_K}{\sqrt{d_{qk}}}
    $$

    其中$P=Q\cdot K^T$; $+$意味着原始Attention机制中的内容
    
    效果：证明了$Y$的去中心化范数(大概能衡量秩的大小)相比依旧是三次速度
## 未来计划

- 哪些模型具备有区分不相关和负相关的能力？这些模型有什么特征使得他们具备这种能力？

没有相关工作

- DNN中有哪些工作是使用影响因子模型代替概率模型的？这么做带来了什么好处
- 哪些工作定量分析了Attention/DNN中的删除操作？

感谢你的访问，期待与你交流！
- 表示的演化和坍缩之间的区别

